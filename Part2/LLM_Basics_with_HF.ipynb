{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfsDR_omdNea"
   },
   "source": [
    "# LLM Basics with Hugging Face\n",
    "This notebook demonstrates how to load LLM models by utilizing Hugging Face, and how to make queries.\n",
    "<!--table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Gemma_Basics_with_HF.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Adapted for EECE.4860/5860 at UMass Lowell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaqZItBdeokU"
   },
   "source": [
    "## Prerequisites \n",
    "\n",
    "### HuggingFace setup\n",
    "\n",
    "Before we dive into the tutorial, let's get you set up with HuggingFace:\n",
    "\n",
    "1. **Hugging Face Account:**  If you don't already have one, you can create a free Hugging Face account by clicking [here](https://huggingface.co/join).\n",
    "2. **LLM Model Access:** Head over to the [Gemma model page](https://huggingface.co/google/gemma-2b) and [llama2 model papge](https://huggingface.co/meta-llama/Llama-2-7b-hf) and accept the usage conditions.\n",
    "3. **Hugging Face Token:**  You need to create a token on HuggingFace and use it to login from this notebook. Once you are logged in, you can download the models. Check [this guide](https://huggingface.co/docs/hub/en/security-tokens) on how to create a token on HF. Generate a Hugging Face access (preferably `write` permission) token by clicking [here](https://huggingface.co/settings/tokens). **Save the token in a safe document that you can access**. Once you've completed these steps, you're ready to move on to the next section where we'll install necessary packages and log into HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFLddpGeaKh5"
   },
   "source": [
    "**If there is no error in the previous step, you are all set and ready to explore the possibilities with LLM models!**\n",
    "\n",
    "\n",
    "**You need to click the next cell to proceed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXFZFUJHgTcU"
   },
   "source": [
    "## Instantiate the Gemma 2B model (or other models)\n",
    "\n",
    "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.\n",
    "\n",
    "Please note we list here a few variants of the Gemma models for you to play with.\n",
    "\n",
    "Other models is this example include Llama 2 from Meta.\n",
    "\n",
    "Let's get started by loading the model from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_bahJBmwvSp"
   },
   "source": [
    "### Log into Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GIFFCHi-wvSp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade54d7258254d86af86cf8ba38170a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you could use OS env variable to store the HF token\n",
    "#from huggingface_hub import login\n",
    "#login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# or use an input box on this notebook to copy/paste the token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgl8ZjHpwvSq"
   },
   "source": [
    "### Loading the model from HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w_z4600bwvSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "74tpQWWWwvSq"
   },
   "outputs": [],
   "source": [
    "# Let's load the tokenizer first\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UD-eXTxxwvSq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/genai-labs/lib/python3.12/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac4a4651bad4b0e8f92a3002e762791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# We could typically quantize the model to reduce its weight\n",
    "# But to simplify the process, we won't quantize it in this notebook\n",
    "\n",
    "# Let's load the chosen model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyw7fwOGwvSq"
   },
   "source": [
    "### Trying it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favourite color is blue. \n",
      "\n",
      "I love the way it makes me feel calm and peaceful. \n",
      "\n",
      "It'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My favourite color is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=20)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nrVBVTtlwvSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who won the 2016 baseball World Series? Answer: The Chicago Cubs. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who won the 2016 baseball World Series? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=40)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zYT6m2LNvxdo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can you use an LLM for? Answer: \n",
      "\n",
      "LLMs (Large Language Models) are powerful tools with a wide range of applications. Here are some examples:\n",
      "\n",
      "**Content Creation:**\n",
      "* **Writing:** Generating articles, blog posts, social media content, marketing copy, and more.\n",
      "* **Translation:** Translating text between languages.\n",
      "* **Summarization:** Condensing large amounts of text into concise summaries.\n",
      "* **Code Generation:** Writing code in various programming languages.\n",
      "\n",
      "**Research and Education:**\n",
      "* **Research Assistance:** Finding relevant research papers and articles.\n",
      "* **Tutoring:** Providing explanations and answering questions.\n",
      "* **Personalized Learning:** Tailoring educational content to individual needs.\n",
      "\n",
      "**Customer Service and Business:**\n",
      "* **Chatbots:** Providing automated customer support.\n",
      "* **Email Automation:** Automating email responses and marketing campaigns.\n",
      "* **Data Analysis:** Extracting insights from large datasets.\n",
      "\n",
      "**Entertainment and Creativity:**\n",
      "* **Storytelling:** Generating creative stories and narratives.\n",
      "* **Poetry and Songwriting:** Creating poems, lyrics, and musical pieces.\n",
      "* **Game Development:** Building interactive game elements and storylines.\n",
      "\n",
      "**Other Applications:**\n",
      "* **Accessibility:** Providing text-to-speech and speech-to-text capabilities.\n",
      "* **Personalization:** Tailoring experiences to individual preferences.\n",
      "* **Security:** Detecting and preventing malicious activities.\n",
      "\n",
      "\n",
      "**It's important to note that LLMs are still under development and have limitations. They can sometimes generate inaccurate or biased information, and it's crucial to use them responsibly and critically.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What can you use an LLM for? Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want a list of winners and years of the baseball World Series Championships from 2010 to 2025 in chronological order with an explanation only for the year 2026. Generate a json output that lists the winners and their years from 2010 to 2026. Answer:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"WorldSeriesWinners\": [\n",
      "    {\n",
      "      \"Year\": 2010,\n",
      "      \"Winner\": \"San Francisco Giants\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2011,\n",
      "      \"Winner\": \"Texas Rangers\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2012,\n",
      "      \"Winner\": \"St. Louis Cardinals\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2013,\n",
      "      \"Winner\": \"Boston Red Sox\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2014,\n",
      "      \"Winner\": \"Kansas City Royals\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2015,\n",
      "      \"Winner\": \"Kansas City Royals\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2016,\n",
      "      \"Winner\": \"Chicago Cubs\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2017,\n",
      "      \"Winner\": \"Houston Astros\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2018,\n",
      "      \"Winner\": \"Boston Red Sox\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2019,\n",
      "      \"Winner\": \"Washington Nationals\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2020,\n",
      "      \"Winner\": \"Los Angeles Dodgers\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2021,\n",
      "      \"Winner\": \"Atlanta Braves\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2022,\n",
      "      \"Winner\": \"Houston Astros\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2023,\n",
      "      \"Winner\": \"Philadelphia Phillies\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2024,\n",
      "      \"Winner\": \"Arizona Diamondbacks\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2025,\n",
      "      \"Winner\": \"Los Angeles Dodgers\"\n",
      "    },\n",
      "    {\n",
      "      \"Year\": 2026,\n",
      "      \"Winner\": \"TBD\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation for 2026:**\n",
      "\n",
      "The World Series in 2026 is not yet complete. The MLB season is ongoing, and the winner will be determined in the Fall.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I want a list of winners and years of the baseball World Series Championships from 2010 to 2025 in chronological order with an explanation only for the year 2026. Generate a json output that lists the winners and their years from 2010 to 2026. Answer:\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Gemma_Basics_with_HF.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
